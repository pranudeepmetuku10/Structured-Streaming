{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fab6ac",
   "metadata": {},
   "source": [
    "# 04 - Streaming Aggregations\n",
    "\n",
    "## Overview\n",
    "This notebook performs streaming aggregations on the transformed transaction data using Spark SQL.\n",
    "\n",
    "## Aggregation Patterns\n",
    "Structured Streaming supports two main aggregation patterns:\n",
    "\n",
    "1. **Stateless Aggregations**: Simple groupBy operations without time windows\n",
    "2. **Stateful Aggregations**: Time-windowed aggregations that maintain state across micro-batches\n",
    "\n",
    "## Use Cases\n",
    "- Real-time KPIs and metrics\n",
    "- Windowed analytics (hourly, daily)\n",
    "- Customer behavior analysis\n",
    "- Revenue tracking by segment\n",
    "\n",
    "## Output Modes\n",
    "- **Complete**: Output entire result table (used for aggregations without watermarks)\n",
    "- **Update**: Output only rows that changed\n",
    "- **Append**: Output only new rows (requires watermarking for aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e223f",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get existing Spark session or create new one\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise Exception(\"No active session\")\n",
    "    print(\"Using existing Spark session\")\n",
    "except:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TransactionStreamingETL\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Created new Spark session\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd702d",
   "metadata": {},
   "source": [
    "## Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "SQL_DIR = BASE_DIR / 'sql'\n",
    "INPUT_DIR = str(BASE_DIR / 'data' / 'input')\n",
    "\n",
    "print(f\"SQL Directory: {SQL_DIR}\")\n",
    "print(f\"Input Directory: {INPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4c77e",
   "metadata": {},
   "source": [
    "## Ensure Transformed View Exists\n",
    "\n",
    "Verify that the transformed_transactions view is available from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7071d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Check if transformed view exists\n",
    "existing_views = [table.name for table in spark.catalog.listTables() if table.isTemporary]\n",
    "\n",
    "if 'transformed_transactions' not in existing_views:\n",
    "    print(\"Setting up data pipeline from scratch...\")\n",
    "    \n",
    "    # Define schema\n",
    "    transaction_schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), False),\n",
    "        StructField(\"user_id\", StringType(), False),\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"product_category\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"payment_method\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"event_time\", StringType(), False),\n",
    "        StructField(\"country_code\", StringType(), True),\n",
    "        StructField(\"discount_percent\", DoubleType(), True),\n",
    "        StructField(\"customer_segment\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create raw stream\n",
    "    raw_stream = spark.readStream \\\n",
    "        .format(\"csv\") \\\n",
    "        .schema(transaction_schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .load(INPUT_DIR)\n",
    "    \n",
    "    raw_stream.createOrReplaceTempView(\"raw_transactions\")\n",
    "    \n",
    "    # Load and apply transformations\n",
    "    with open(SQL_DIR / 'transformations.sql', 'r') as f:\n",
    "        transformation_sql = f.read()\n",
    "    \n",
    "    transformed_stream = spark.sql(transformation_sql)\n",
    "    transformed_stream.createOrReplaceTempView(\"transformed_transactions\")\n",
    "    \n",
    "    print(\"Pipeline setup complete\")\n",
    "else:\n",
    "    print(\"Using existing 'transformed_transactions' view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a0812",
   "metadata": {},
   "source": [
    "## Load Aggregation SQL\n",
    "\n",
    "Load aggregation queries from the external SQL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQL from external file\n",
    "sql_file_path = SQL_DIR / 'aggregations.sql'\n",
    "\n",
    "with open(sql_file_path, 'r') as f:\n",
    "    aggregation_sql = f.read()\n",
    "\n",
    "print(f\"Loaded SQL from: {sql_file_path}\")\n",
    "print(f\"\\nSQL Query ({len(aggregation_sql)} characters):\")\n",
    "print(\"=\" * 80)\n",
    "print(aggregation_sql)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480dbf1d",
   "metadata": {},
   "source": [
    "## Execute Aggregation Query\n",
    "\n",
    "Apply the SQL aggregation to create streaming analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute aggregation SQL\n",
    "aggregated_stream = spark.sql(aggregation_sql)\n",
    "\n",
    "print(\"Aggregation applied successfully!\")\n",
    "print(f\"Is Streaming: {aggregated_stream.isStreaming}\")\n",
    "print(f\"\\nAggregated Schema:\")\n",
    "aggregated_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3794c722",
   "metadata": {},
   "source": [
    "## Register Aggregated View\n",
    "\n",
    "Make the aggregated results available as a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75935c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register aggregated stream\n",
    "aggregated_stream.createOrReplaceTempView(\"transaction_metrics\")\n",
    "\n",
    "print(\"Registered as 'transaction_metrics' view\")\n",
    "print(\"This view contains real-time aggregated KPIs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a7e44",
   "metadata": {},
   "source": [
    "## Test Aggregation Output\n",
    "\n",
    "Display the aggregated metrics to verify calculations.\n",
    "\n",
    "**Note**: We use `complete` output mode for aggregations without watermarks, which outputs the entire result table on each trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write aggregated results to console\n",
    "aggregation_test = aggregated_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 50) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Aggregation query started...\")\n",
    "print(f\"Query ID: {aggregation_test.id}\")\n",
    "print(f\"Output Mode: complete\")\n",
    "print(\"\\nNote: Complete mode outputs the full result table on each trigger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e24ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it run to show results\n",
    "import time\n",
    "time.sleep(25)\n",
    "\n",
    "# Stop test query\n",
    "aggregation_test.stop()\n",
    "print(\"Aggregation test stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e65139",
   "metadata": {},
   "source": [
    "## Create Additional Analytics Views\n",
    "\n",
    "Generate secondary metrics by querying the aggregated view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top performing categories by revenue\n",
    "top_categories_sql = \"\"\"\n",
    "SELECT \n",
    "    product_category,\n",
    "    total_revenue,\n",
    "    total_transactions,\n",
    "    avg_transaction_value,\n",
    "    high_value_transactions\n",
    "FROM transaction_metrics\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_categories = spark.sql(top_categories_sql)\n",
    "\n",
    "print(\"Created top categories view\")\n",
    "print(f\"Is Streaming: {top_categories.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55777bc0",
   "metadata": {},
   "source": [
    "## Monitor Stream Health\n",
    "\n",
    "Check the status of all active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a12323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all active streaming queries\n",
    "active_streams = spark.streams.active\n",
    "print(f\"Active Streams: {len(active_streams)}\\n\")\n",
    "\n",
    "for stream in active_streams:\n",
    "    print(f\"Stream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Status: {stream.status['message']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f410505",
   "metadata": {},
   "source": [
    "## Aggregation Columns Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display aggregated columns\n",
    "print(\"Aggregated Metrics Available:\")\n",
    "for col_name in aggregated_stream.columns:\n",
    "    print(f\"  - {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef3c7b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. Loaded aggregation logic from external SQL file\n",
    "2. Performed streaming aggregations using GroupBy\n",
    "3. Calculated real-time KPIs and metrics\n",
    "4. Used complete output mode for stateful aggregations\n",
    "5. Created derived analytics views\n",
    "\n",
    "**Key Metrics Calculated:**\n",
    "- Total transaction counts by category\n",
    "- Revenue totals and averages\n",
    "- High-value transaction counts\n",
    "- Completion rates\n",
    "- Unique customer counts\n",
    "\n",
    "**Aggregation Considerations:**\n",
    "- **Complete mode**: Required for aggregations without event-time watermarks\n",
    "- **State management**: Spark maintains state across micro-batches\n",
    "- **Memory**: Complete mode can be memory-intensive for large cardinality\n",
    "- **Production**: Consider windowed aggregations with watermarks for append mode\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to notebook 05 to write results to sink\n",
    "- Configure checkpointing for fault tolerance\n",
    "- Write aggregated data to Parquet format\n",
    "- Set up monitoring and alerting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
