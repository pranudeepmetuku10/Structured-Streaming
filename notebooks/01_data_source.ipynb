{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92f16e3",
   "metadata": {},
   "source": [
    "# 01 - Data Source Setup\n",
    "\n",
    "## Overview\n",
    "This notebook sets up the streaming data source for our ETL pipeline. We simulate real-time e-commerce transaction data by generating CSV files incrementally.\n",
    "\n",
    "## Business Context\n",
    "We are processing transaction events from an e-commerce platform. Each transaction contains:\n",
    "- Transaction ID and User ID\n",
    "- Product information\n",
    "- Transaction amount\n",
    "- Event timestamp\n",
    "- Transaction status (completed, pending, failed)\n",
    "\n",
    "## Approach\n",
    "We will generate batches of CSV files to simulate a streaming source. In production, this would be replaced with Kafka, Kinesis, or other streaming platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e31d7",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the output directory and data generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "INPUT_DIR = BASE_DIR / 'data' / 'input'\n",
    "OUTPUT_DIR = BASE_DIR / 'data' / 'output'\n",
    "\n",
    "# Ensure directories exist\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Input Directory: {INPUT_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed74f4",
   "metadata": {},
   "source": [
    "## Data Generation Functions\n",
    "\n",
    "Create helper functions to generate realistic transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for generation\n",
    "PRODUCT_CATEGORIES = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports', 'Toys', 'Food', 'Beauty']\n",
    "STATUSES = ['completed', 'pending', 'failed', 'refunded']\n",
    "PAYMENT_METHODS = ['credit_card', 'debit_card', 'paypal', 'bank_transfer']\n",
    "\n",
    "def generate_transaction(transaction_id, base_time):\n",
    "    \"\"\"\n",
    "    Generate a single transaction record.\n",
    "    \n",
    "    Args:\n",
    "        transaction_id: Unique transaction identifier\n",
    "        base_time: Base timestamp for the transaction\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing transaction data\n",
    "    \"\"\"\n",
    "    # Add some randomness to timestamp (within 1 minute)\n",
    "    event_time = base_time + timedelta(seconds=random.randint(0, 60))\n",
    "    \n",
    "    # Generate transaction data with realistic distributions\n",
    "    status = random.choices(\n",
    "        STATUSES, \n",
    "        weights=[0.85, 0.08, 0.05, 0.02]  # Most transactions complete successfully\n",
    "    )[0]\n",
    "    \n",
    "    transaction = {\n",
    "        'transaction_id': f'TXN{transaction_id:08d}',\n",
    "        'user_id': f'USER{random.randint(1000, 9999)}',\n",
    "        'product_id': f'PROD{random.randint(100, 999)}',\n",
    "        'product_category': random.choice(PRODUCT_CATEGORIES),\n",
    "        'amount': round(random.uniform(10.0, 500.0), 2),\n",
    "        'quantity': random.randint(1, 5),\n",
    "        'payment_method': random.choice(PAYMENT_METHODS),\n",
    "        'status': status,\n",
    "        'event_time': event_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'country_code': random.choice(['US', 'UK', 'CA', 'DE', 'FR', 'JP', 'AU']),\n",
    "        # Introduce some data quality issues (nulls, invalid values)\n",
    "        'discount_percent': None if random.random() < 0.3 else round(random.uniform(0, 30), 2),\n",
    "        'customer_segment': random.choice(['premium', 'regular', 'new', None])\n",
    "    }\n",
    "    \n",
    "    return transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439b784",
   "metadata": {},
   "source": [
    "## Generate Streaming Data\n",
    "\n",
    "Create multiple batches of CSV files to simulate streaming data arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49baac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_number, num_records=100):\n",
    "    \"\"\"\n",
    "    Generate a batch of transactions and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        batch_number: Batch identifier\n",
    "        num_records: Number of records to generate\n",
    "    \"\"\"\n",
    "    base_time = datetime.now() - timedelta(hours=24) + timedelta(minutes=batch_number * 5)\n",
    "    start_id = batch_number * num_records\n",
    "    \n",
    "    transactions = [\n",
    "        generate_transaction(start_id + i, base_time) \n",
    "        for i in range(num_records)\n",
    "    ]\n",
    "    \n",
    "    # Write to CSV\n",
    "    filename = INPUT_DIR / f'transactions_batch_{batch_number:04d}.csv'\n",
    "    \n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = transactions[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(transactions)\n",
    "    \n",
    "    print(f\"Generated batch {batch_number}: {filename} ({num_records} records)\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0597d8f",
   "metadata": {},
   "source": [
    "## Execute Data Generation\n",
    "\n",
    "Generate initial batches of data. For demonstration purposes, we'll create 5 batches.\n",
    "In a real streaming scenario, new files would arrive continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up existing files (optional - for fresh runs)\n",
    "print(\"Cleaning up existing input files...\")\n",
    "for file in INPUT_DIR.glob('*.csv'):\n",
    "    file.unlink()\n",
    "print(\"Cleanup complete.\\n\")\n",
    "\n",
    "# Generate initial batches\n",
    "NUM_BATCHES = 5\n",
    "RECORDS_PER_BATCH = 100\n",
    "\n",
    "print(f\"Generating {NUM_BATCHES} batches with {RECORDS_PER_BATCH} records each...\\n\")\n",
    "\n",
    "for batch_num in range(NUM_BATCHES):\n",
    "    generate_batch(batch_num, RECORDS_PER_BATCH)\n",
    "    # Simulate delay between batch arrivals (optional)\n",
    "    if batch_num < NUM_BATCHES - 1:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nData generation complete! Total records: {NUM_BATCHES * RECORDS_PER_BATCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db07302",
   "metadata": {},
   "source": [
    "## Verify Generated Data\n",
    "\n",
    "Inspect the first few records to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "files = sorted(INPUT_DIR.glob('*.csv'))\n",
    "print(f\"Total files generated: {len(files)}\\n\")\n",
    "\n",
    "for file in files:\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df762f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display sample data from first batch\n",
    "import pandas as pd\n",
    "\n",
    "if files:\n",
    "    sample_df = pd.read_csv(files[0])\n",
    "    print(f\"\\nSample data from {files[0].name}:\")\n",
    "    print(f\"\\nShape: {sample_df.shape}\")\n",
    "    print(f\"\\nFirst 5 records:\")\n",
    "    print(sample_df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(sample_df.dtypes)\n",
    "    print(f\"\\nNull counts:\")\n",
    "    print(sample_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b672ac",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully generated streaming transaction data in CSV format. The data includes:\n",
    "\n",
    "- Realistic transaction records with various attributes\n",
    "- Multiple product categories and payment methods\n",
    "- Intentional data quality issues (nulls) to demonstrate cleaning in downstream steps\n",
    "- Timestamp-based ordering to simulate real-time arrival\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to notebook 02 to set up Spark Structured Streaming ingestion\n",
    "- Define explicit schema for type safety\n",
    "- Configure streaming read from this input directory"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
