{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae70307",
   "metadata": {},
   "source": [
    "# 03 - Data Transformations with Spark SQL\n",
    "\n",
    "## Overview\n",
    "This notebook applies data transformations to the streaming transaction data using pure Spark SQL.\n",
    "\n",
    "## Transformation Goals\n",
    "1. Cast string timestamps to proper timestamp type\n",
    "2. Filter out invalid or incomplete records\n",
    "3. Handle null values appropriately\n",
    "4. Create derived columns for analytics\n",
    "5. Standardize data formats\n",
    "\n",
    "## Architecture Pattern\n",
    "All business logic is maintained in external `.sql` files under the `sql/` directory. This approach:\n",
    "- Separates concerns (logic vs orchestration)\n",
    "- Enables SQL-first development\n",
    "- Facilitates version control and testing\n",
    "- Allows non-Python developers to contribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a032e32",
   "metadata": {},
   "source": [
    "## Initialize or Retrieve Spark Session\n",
    "\n",
    "If continuing from notebook 02, retrieve the existing session. Otherwise, create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get existing Spark session or create new one\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise Exception(\"No active session\")\n",
    "    print(\"Using existing Spark session\")\n",
    "except:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TransactionStreamingETL\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Created new Spark session\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b85f4",
   "metadata": {},
   "source": [
    "## Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a792cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "SQL_DIR = BASE_DIR / 'sql'\n",
    "INPUT_DIR = str(BASE_DIR / 'data' / 'input')\n",
    "\n",
    "print(f\"SQL Directory: {SQL_DIR}\")\n",
    "print(f\"Input Directory: {INPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74d4aa",
   "metadata": {},
   "source": [
    "## Set Up Streaming Source\n",
    "\n",
    "If not already set up, recreate the streaming source and temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd75aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Define schema (same as notebook 02)\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"event_time\", StringType(), False),\n",
    "    StructField(\"country_code\", StringType(), True),\n",
    "    StructField(\"discount_percent\", DoubleType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Check if view already exists, otherwise create it\n",
    "existing_views = [table.name for table in spark.catalog.listTables() if table.isTemporary]\n",
    "\n",
    "if 'raw_transactions' not in existing_views:\n",
    "    print(\"Creating streaming source...\")\n",
    "    raw_stream = spark.readStream \\\n",
    "        .format(\"csv\") \\\n",
    "        .schema(transaction_schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .load(INPUT_DIR)\n",
    "    \n",
    "    raw_stream.createOrReplaceTempView(\"raw_transactions\")\n",
    "    print(\"Streaming source created and registered as 'raw_transactions'\")\n",
    "else:\n",
    "    print(\"Using existing 'raw_transactions' view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa49fd",
   "metadata": {},
   "source": [
    "## Load SQL Transformation Query\n",
    "\n",
    "Load the transformation logic from the external SQL file. This file contains all the business rules for cleaning and enriching the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQL from external file\n",
    "sql_file_path = SQL_DIR / 'transformations.sql'\n",
    "\n",
    "with open(sql_file_path, 'r') as f:\n",
    "    transformation_sql = f.read()\n",
    "\n",
    "print(f\"Loaded SQL from: {sql_file_path}\")\n",
    "print(f\"\\nSQL Query ({len(transformation_sql)} characters):\")\n",
    "print(\"=\" * 80)\n",
    "print(transformation_sql)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf8b71",
   "metadata": {},
   "source": [
    "## Execute Transformation\n",
    "\n",
    "Apply the SQL transformation to create a cleaned and enriched streaming DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute transformation SQL\n",
    "transformed_stream = spark.sql(transformation_sql)\n",
    "\n",
    "print(\"Transformation applied successfully!\")\n",
    "print(f\"Is Streaming: {transformed_stream.isStreaming}\")\n",
    "print(f\"\\nTransformed Schema:\")\n",
    "transformed_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46931a55",
   "metadata": {},
   "source": [
    "## Register Transformed View\n",
    "\n",
    "Register the transformed data as a new temporary view for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff270671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register transformed stream as temporary view\n",
    "transformed_stream.createOrReplaceTempView(\"transformed_transactions\")\n",
    "\n",
    "print(\"Registered as 'transformed_transactions' view\")\n",
    "print(\"This view is ready for aggregations and analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7177d85",
   "metadata": {},
   "source": [
    "## Validate Transformations\n",
    "\n",
    "Execute a test query to verify the transformations are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query to validate transformations\n",
    "validation_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        user_id,\n",
    "        product_category,\n",
    "        amount,\n",
    "        discounted_amount,\n",
    "        revenue,\n",
    "        is_high_value,\n",
    "        status,\n",
    "        event_timestamp,\n",
    "        event_date,\n",
    "        event_hour\n",
    "    FROM transformed_transactions\n",
    "\"\"\")\n",
    "\n",
    "print(\"Validation query created\")\n",
    "print(f\"Is Streaming: {validation_query.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba609d",
   "metadata": {},
   "source": [
    "## Run Test Output\n",
    "\n",
    "Display sample transformed records to console for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d67d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to console for validation\n",
    "validation_stream = validation_query.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Validation query started...\")\n",
    "print(f\"Query ID: {validation_stream.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it run briefly\n",
    "import time\n",
    "time.sleep(20)\n",
    "\n",
    "# Stop validation query\n",
    "validation_stream.stop()\n",
    "print(\"Validation query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600cd35",
   "metadata": {},
   "source": [
    "## Transformation Summary\n",
    "\n",
    "Review what transformations were applied by querying the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column list\n",
    "print(\"Transformed Columns:\")\n",
    "for col_name in transformed_stream.columns:\n",
    "    print(f\"  - {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63356958",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. Loaded SQL transformation logic from external file\n",
    "2. Applied data cleaning and validation rules\n",
    "3. Cast timestamp strings to proper timestamp types\n",
    "4. Created derived columns (revenue, flags, date parts)\n",
    "5. Handled null values with COALESCE\n",
    "6. Registered transformed data as SQL view\n",
    "\n",
    "**Key Transformations Applied:**\n",
    "- Timestamp parsing and date extraction\n",
    "- Revenue calculation with discount logic\n",
    "- High-value transaction flagging\n",
    "- Status categorization\n",
    "- Invalid record filtering\n",
    "\n",
    "**Benefits of SQL-Based Transformations:**\n",
    "- Declarative and readable\n",
    "- Catalyst optimizer handles execution\n",
    "- Version controlled separately\n",
    "- Easy to test and modify\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to notebook 04 for aggregations\n",
    "- Calculate windowed metrics and KPIs\n",
    "- Perform stateful operations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
