{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fc9931",
   "metadata": {},
   "source": [
    "# 02 - Streaming Ingestion\n",
    "\n",
    "## Overview\n",
    "This notebook sets up Spark Structured Streaming to read transaction data from the input directory.\n",
    "\n",
    "## Key Concepts\n",
    "- **Structured Streaming**: Spark's declarative streaming API built on DataFrames\n",
    "- **Explicit Schema**: Required for production streaming to ensure type safety and performance\n",
    "- **File Source**: We use CSV file streaming, which monitors a directory for new files\n",
    "\n",
    "## Why Explicit Schema?\n",
    "In production streaming, we avoid `inferSchema` because:\n",
    "1. Performance: Schema inference requires reading data twice\n",
    "2. Correctness: Ensures consistent types across batches\n",
    "3. Safety: Catches schema violations early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6271e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38abb1c7",
   "metadata": {},
   "source": [
    "## Initialize Spark Session\n",
    "\n",
    "Create SparkSession with appropriate configurations for structured streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionStreamingETL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.streaming.stateStore.providerClass\", \n",
    "            \"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31390daf",
   "metadata": {},
   "source": [
    "## Define Schema\n",
    "\n",
    "Explicitly define the schema for our transaction data. This is critical for production streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit schema for transaction data\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),      # Primary key - not nullable\n",
    "    StructField(\"user_id\", StringType(), False),             # Customer identifier\n",
    "    StructField(\"product_id\", StringType(), False),          # Product identifier\n",
    "    StructField(\"product_category\", StringType(), True),     # Product category\n",
    "    StructField(\"amount\", DoubleType(), False),              # Transaction amount\n",
    "    StructField(\"quantity\", IntegerType(), False),           # Item quantity\n",
    "    StructField(\"payment_method\", StringType(), True),       # Payment type\n",
    "    StructField(\"status\", StringType(), False),              # Transaction status\n",
    "    StructField(\"event_time\", StringType(), False),          # Will be cast to timestamp later\n",
    "    StructField(\"country_code\", StringType(), True),         # Country code\n",
    "    StructField(\"discount_percent\", DoubleType(), True),     # Discount applied (nullable)\n",
    "    StructField(\"customer_segment\", StringType(), True)      # Customer segment (nullable)\n",
    "])\n",
    "\n",
    "print(\"Schema defined successfully:\")\n",
    "print(transaction_schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfaa642",
   "metadata": {},
   "source": [
    "## Configure Paths\n",
    "\n",
    "Set up directory paths for reading streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "INPUT_DIR = str(BASE_DIR / 'data' / 'input')\n",
    "OUTPUT_DIR = str(BASE_DIR / 'data' / 'output')\n",
    "\n",
    "print(f\"Input Directory: {INPUT_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67818c",
   "metadata": {},
   "source": [
    "## Read Streaming Data\n",
    "\n",
    "Set up the streaming DataFrame using `readStream`. This creates a streaming source that monitors the input directory for new CSV files.\n",
    "\n",
    "### Streaming Options Explained:\n",
    "- **maxFilesPerTrigger**: Controls the rate of file processing (throttling)\n",
    "- **cleanSource**: Optionally archives processed files to prevent reprocessing\n",
    "- **header**: Specifies that CSV files have headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed24ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming DataFrame\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(transaction_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(INPUT_DIR)\n",
    "\n",
    "print(\"Streaming DataFrame created successfully!\")\n",
    "print(f\"Is Streaming: {raw_stream.isStreaming}\")\n",
    "print(f\"\\nSchema:\")\n",
    "raw_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c3020",
   "metadata": {},
   "source": [
    "## Register as Temporary View\n",
    "\n",
    "Register the streaming DataFrame as a temporary SQL view. This enables us to use Spark SQL for all transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register streaming DataFrame as temporary view\n",
    "raw_stream.createOrReplaceTempView(\"raw_transactions\")\n",
    "\n",
    "print(\"Registered streaming DataFrame as 'raw_transactions' view\")\n",
    "print(\"This view can now be queried using Spark SQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20686b8",
   "metadata": {},
   "source": [
    "## Test Query (Console Output)\n",
    "\n",
    "Execute a simple streaming query to verify data ingestion. This uses console output mode for testing.\n",
    "\n",
    "**Note:** This is a test query. In production, we would not use console output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query - display first few records\n",
    "test_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        user_id,\n",
    "        product_category,\n",
    "        amount,\n",
    "        status,\n",
    "        event_time\n",
    "    FROM raw_transactions\n",
    "\"\"\")\n",
    "\n",
    "# Write to console for verification (will run briefly)\n",
    "console_query = test_query.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"Console output query started...\")\n",
    "print(\"Query ID:\", console_query.id)\n",
    "print(\"Query Name:\", console_query.name)\n",
    "print(\"\\nNote: This query will run for 20 seconds for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it run briefly to see output\n",
    "import time\n",
    "time.sleep(20)\n",
    "\n",
    "# Stop the test query\n",
    "console_query.stop()\n",
    "print(\"Test query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b6d42",
   "metadata": {},
   "source": [
    "## Streaming Query Metrics\n",
    "\n",
    "Check the status and metrics of active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all active streaming queries\n",
    "active_streams = spark.streams.active\n",
    "print(f\"Active Streams: {len(active_streams)}\")\n",
    "\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nStream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccadb473",
   "metadata": {},
   "source": [
    "## Verify Schema Compliance\n",
    "\n",
    "Demonstrate that our schema is correctly applied and type-safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ddd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data types\n",
    "print(\"Column Data Types:\")\n",
    "for field in raw_stream.schema.fields:\n",
    "    nullable = \"NULL\" if field.nullable else \"NOT NULL\"\n",
    "    print(f\"  {field.name:<20} {str(field.dataType):<15} {nullable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223099d2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. Initialized Spark with streaming configurations\n",
    "2. Defined an explicit schema for type safety\n",
    "3. Created a streaming DataFrame from CSV files\n",
    "4. Registered the stream as a SQL temporary view\n",
    "5. Verified data ingestion with a test query\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Explicit schema is mandatory for production streaming\n",
    "- `readStream` continuously monitors for new files\n",
    "- Streaming DataFrames can be registered as SQL views\n",
    "- `isStreaming=true` indicates this is a streaming DataFrame\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to notebook 03 for data transformations using Spark SQL\n",
    "- Apply data cleaning, type casting, and business logic\n",
    "- Load SQL queries from external files"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
