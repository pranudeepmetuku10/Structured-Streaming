{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b813bd9e",
   "metadata": {},
   "source": [
    "# 05 - Sink and Output\n",
    "\n",
    "## Overview\n",
    "This notebook writes the transformed and aggregated streaming data to persistent storage using Parquet format.\n",
    "\n",
    "## Sink Configuration\n",
    "We will configure streaming sinks with:\n",
    "- **Checkpointing**: Ensures exactly-once processing semantics\n",
    "- **Parquet format**: Columnar storage for efficient analytics\n",
    "- **Partitioning**: Organizes data by date for optimal queries\n",
    "- **Trigger intervals**: Controls micro-batch processing frequency\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### Checkpointing\n",
    "Checkpoints store:\n",
    "- Stream offset information\n",
    "- Metadata about processed data\n",
    "- State store data for aggregations\n",
    "\n",
    "This enables fault-tolerance and exactly-once guarantees.\n",
    "\n",
    "### Output Modes\n",
    "- **Append**: Only new rows (best for most ETL scenarios)\n",
    "- **Complete**: Entire result table (for aggregations)\n",
    "- **Update**: Only changed rows (requires Delta Lake or similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4ccfe",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0145316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get existing Spark session or create new one\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise Exception(\"No active session\")\n",
    "    print(\"Using existing Spark session\")\n",
    "except:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TransactionStreamingETL\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Created new Spark session\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ec599",
   "metadata": {},
   "source": [
    "## Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd35b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "BASE_DIR = Path(os.path.abspath('')).parent\n",
    "SQL_DIR = BASE_DIR / 'sql'\n",
    "INPUT_DIR = str(BASE_DIR / 'data' / 'input')\n",
    "OUTPUT_DIR = str(BASE_DIR / 'data' / 'output')\n",
    "\n",
    "# Output subdirectories\n",
    "TRANSFORMED_OUTPUT = f\"{OUTPUT_DIR}/transformed_transactions\"\n",
    "AGGREGATED_OUTPUT = f\"{OUTPUT_DIR}/transaction_metrics\"\n",
    "\n",
    "# Checkpoint directories\n",
    "TRANSFORMED_CHECKPOINT = f\"{OUTPUT_DIR}/checkpoints/transformed\"\n",
    "AGGREGATED_CHECKPOINT = f\"{OUTPUT_DIR}/checkpoints/aggregated\"\n",
    "\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Transformed Output: {TRANSFORMED_OUTPUT}\")\n",
    "print(f\"Aggregated Output: {AGGREGATED_OUTPUT}\")\n",
    "print(f\"\\nCheckpoint Locations:\")\n",
    "print(f\"  Transformed: {TRANSFORMED_CHECKPOINT}\")\n",
    "print(f\"  Aggregated: {AGGREGATED_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b48df",
   "metadata": {},
   "source": [
    "## Clean Previous Outputs (Optional)\n",
    "\n",
    "For fresh runs, clean up existing output and checkpoint directories.\n",
    "\n",
    "**Warning**: In production, never delete checkpoints as this will cause data loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous outputs (optional - for demo purposes)\n",
    "def clean_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Cleaned: {path}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Uncomment to clean (use with caution)\n",
    "# clean_directory(TRANSFORMED_OUTPUT)\n",
    "# clean_directory(AGGREGATED_OUTPUT)\n",
    "# clean_directory(TRANSFORMED_CHECKPOINT)\n",
    "# clean_directory(AGGREGATED_CHECKPOINT)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(TRANSFORMED_OUTPUT, exist_ok=True)\n",
    "os.makedirs(AGGREGATED_OUTPUT, exist_ok=True)\n",
    "os.makedirs(TRANSFORMED_CHECKPOINT, exist_ok=True)\n",
    "os.makedirs(AGGREGATED_CHECKPOINT, exist_ok=True)\n",
    "\n",
    "print(\"Output directories ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff687e9e",
   "metadata": {},
   "source": [
    "## Set Up Complete Pipeline\n",
    "\n",
    "Ensure all views are available by running the complete pipeline if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Check existing views\n",
    "existing_views = [table.name for table in spark.catalog.listTables() if table.isTemporary]\n",
    "\n",
    "if 'transformed_transactions' not in existing_views or 'transaction_metrics' not in existing_views:\n",
    "    print(\"Setting up complete pipeline...\")\n",
    "    \n",
    "    # Define schema\n",
    "    transaction_schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), False),\n",
    "        StructField(\"user_id\", StringType(), False),\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"product_category\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"payment_method\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"event_time\", StringType(), False),\n",
    "        StructField(\"country_code\", StringType(), True),\n",
    "        StructField(\"discount_percent\", DoubleType(), True),\n",
    "        StructField(\"customer_segment\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create raw stream\n",
    "    raw_stream = spark.readStream \\\n",
    "        .format(\"csv\") \\\n",
    "        .schema(transaction_schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .load(INPUT_DIR)\n",
    "    \n",
    "    raw_stream.createOrReplaceTempView(\"raw_transactions\")\n",
    "    \n",
    "    # Apply transformations\n",
    "    with open(SQL_DIR / 'transformations.sql', 'r') as f:\n",
    "        transformation_sql = f.read()\n",
    "    \n",
    "    transformed_stream = spark.sql(transformation_sql)\n",
    "    transformed_stream.createOrReplaceTempView(\"transformed_transactions\")\n",
    "    \n",
    "    # Apply aggregations\n",
    "    with open(SQL_DIR / 'aggregations.sql', 'r') as f:\n",
    "        aggregation_sql = f.read()\n",
    "    \n",
    "    aggregated_stream = spark.sql(aggregation_sql)\n",
    "    aggregated_stream.createOrReplaceTempView(\"transaction_metrics\")\n",
    "    \n",
    "    print(\"Pipeline setup complete\")\n",
    "else:\n",
    "    print(\"Using existing views\")\n",
    "    transformed_stream = spark.table(\"transformed_transactions\")\n",
    "    aggregated_stream = spark.table(\"transaction_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e524fd1",
   "metadata": {},
   "source": [
    "## Sink 1: Write Transformed Data\n",
    "\n",
    "Write the transformed transaction data to Parquet with partitioning by date.\n",
    "\n",
    "### Configuration:\n",
    "- **Format**: Parquet (columnar, compressed)\n",
    "- **Output Mode**: Append (write only new records)\n",
    "- **Partitioning**: By event_date for query optimization\n",
    "- **Checkpointing**: Enabled for fault tolerance\n",
    "- **Trigger**: Process available data every 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transformed stream\n",
    "transformed_stream = spark.table(\"transformed_transactions\")\n",
    "\n",
    "# Write transformed data to Parquet\n",
    "transformed_query = transformed_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", TRANSFORMED_OUTPUT) \\\n",
    "    .option(\"checkpointLocation\", TRANSFORMED_CHECKPOINT) \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .queryName(\"TransformedTransactionsWriter\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Transformed data streaming query started!\")\n",
    "print(f\"Query ID: {transformed_query.id}\")\n",
    "print(f\"Query Name: {transformed_query.name}\")\n",
    "print(f\"Output Path: {TRANSFORMED_OUTPUT}\")\n",
    "print(f\"Checkpoint: {TRANSFORMED_CHECKPOINT}\")\n",
    "print(f\"Partitioned by: event_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16949b92",
   "metadata": {},
   "source": [
    "## Sink 2: Write Aggregated Metrics\n",
    "\n",
    "Write the aggregated metrics to Parquet.\n",
    "\n",
    "### Configuration:\n",
    "- **Format**: Parquet\n",
    "- **Output Mode**: Complete (full result table for aggregations)\n",
    "- **Checkpointing**: Enabled\n",
    "- **Trigger**: Process available data every 60 seconds\n",
    "\n",
    "**Note**: Complete mode is used because we're not using watermarks. In production, consider windowed aggregations with append mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated stream\n",
    "aggregated_stream = spark.table(\"transaction_metrics\")\n",
    "\n",
    "# Write aggregated data to Parquet\n",
    "aggregated_query = aggregated_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"path\", AGGREGATED_OUTPUT) \\\n",
    "    .option(\"checkpointLocation\", AGGREGATED_CHECKPOINT) \\\n",
    "    .trigger(processingTime='60 seconds') \\\n",
    "    .queryName(\"AggregatedMetricsWriter\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Aggregated metrics streaming query started!\")\n",
    "print(f\"Query ID: {aggregated_query.id}\")\n",
    "print(f\"Query Name: {aggregated_query.name}\")\n",
    "print(f\"Output Path: {AGGREGATED_OUTPUT}\")\n",
    "print(f\"Checkpoint: {AGGREGATED_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02baa5",
   "metadata": {},
   "source": [
    "## Monitor Streaming Queries\n",
    "\n",
    "Check the status and progress of all active streaming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all active queries\n",
    "print(\"Active Streaming Queries:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"\\nQuery Name: {stream.name}\")\n",
    "    print(f\"Query ID: {stream.id}\")\n",
    "    print(f\"Status: {stream.status['message']}\")\n",
    "    print(f\"Is Active: {stream.isActive}\")\n",
    "    \n",
    "    # Get recent progress if available\n",
    "    if stream.lastProgress:\n",
    "        progress = stream.lastProgress\n",
    "        print(f\"Batch ID: {progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"Input Rows: {progress.get('numInputRows', 0)}\")\n",
    "        print(f\"Processing Rate: {progress.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa115dc1",
   "metadata": {},
   "source": [
    "## Run for Demonstration\n",
    "\n",
    "Let the queries run for a period to process data and write to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9814804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Run for 2 minutes to process data\n",
    "runtime_seconds = 120\n",
    "print(f\"Running streaming queries for {runtime_seconds} seconds...\")\n",
    "print(\"Processing data and writing to Parquet...\\n\")\n",
    "\n",
    "for i in range(0, runtime_seconds, 30):\n",
    "    time.sleep(30)\n",
    "    elapsed = i + 30\n",
    "    print(f\"[{elapsed}s] Queries running... Check output directories for results.\")\n",
    "    \n",
    "    # Show progress\n",
    "    for stream in spark.streams.active:\n",
    "        if stream.lastProgress:\n",
    "            progress = stream.lastProgress\n",
    "            batch_id = progress.get('batchId', 'N/A')\n",
    "            num_rows = progress.get('numInputRows', 0)\n",
    "            print(f\"  {stream.name}: Batch {batch_id}, {num_rows} rows processed\")\n",
    "\n",
    "print(\"\\nStreaming queries have been running. Check output for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f0f64",
   "metadata": {},
   "source": [
    "## Verify Output\n",
    "\n",
    "Check that data has been written to the output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba506b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "print(\"Output Files:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. Transformed Transactions: {TRANSFORMED_OUTPUT}\")\n",
    "if os.path.exists(TRANSFORMED_OUTPUT):\n",
    "    for root, dirs, files in os.walk(TRANSFORMED_OUTPUT):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                size = os.path.getsize(filepath)\n",
    "                print(f\"  {filepath.replace(TRANSFORMED_OUTPUT, '')} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\n2. Aggregated Metrics: {AGGREGATED_OUTPUT}\")\n",
    "if os.path.exists(AGGREGATED_OUTPUT):\n",
    "    for root, dirs, files in os.walk(AGGREGATED_OUTPUT):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                size = os.path.getsize(filepath)\n",
    "                print(f\"  {filepath.replace(AGGREGATED_OUTPUT, '')} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ff9a9",
   "metadata": {},
   "source": [
    "## Read Back Data for Verification\n",
    "\n",
    "Verify written data by reading it back as batch DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transformed data back\n",
    "if os.path.exists(TRANSFORMED_OUTPUT):\n",
    "    transformed_df = spark.read.parquet(TRANSFORMED_OUTPUT)\n",
    "    print(\"Transformed Transactions:\")\n",
    "    print(f\"Total Records: {transformed_df.count()}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    transformed_df.printSchema()\n",
    "    print(f\"\\nSample Data:\")\n",
    "    transformed_df.show(5, truncate=False)\n",
    "else:\n",
    "    print(\"No transformed data found yet. Wait for queries to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83466d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read aggregated data back\n",
    "if os.path.exists(AGGREGATED_OUTPUT):\n",
    "    aggregated_df = spark.read.parquet(AGGREGATED_OUTPUT)\n",
    "    print(\"Aggregated Metrics:\")\n",
    "    print(f\"Total Records: {aggregated_df.count()}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    aggregated_df.printSchema()\n",
    "    print(f\"\\nSample Data:\")\n",
    "    aggregated_df.orderBy(\"total_revenue\", ascending=False).show(10, truncate=False)\n",
    "else:\n",
    "    print(\"No aggregated data found yet. Wait for queries to process data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae542c7e",
   "metadata": {},
   "source": [
    "## Stop Streaming Queries\n",
    "\n",
    "Gracefully stop all active streaming queries.\n",
    "\n",
    "**Note**: In production, queries would run continuously until explicitly stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e477e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all streaming queries\n",
    "print(\"Stopping all streaming queries...\\n\")\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping: {stream.name} (ID: {stream.id})\")\n",
    "    stream.stop()\n",
    "    print(f\"  Stopped successfully\")\n",
    "\n",
    "print(\"\\nAll streaming queries stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952817bd",
   "metadata": {},
   "source": [
    "## Checkpoint Inspection\n",
    "\n",
    "Examine checkpoint directories to understand fault tolerance mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List checkpoint contents\n",
    "print(\"Checkpoint Directories:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for checkpoint_dir in [TRANSFORMED_CHECKPOINT, AGGREGATED_CHECKPOINT]:\n",
    "    print(f\"\\n{checkpoint_dir}:\")\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        for item in os.listdir(checkpoint_dir):\n",
    "            item_path = os.path.join(checkpoint_dir, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"  [DIR]  {item}\")\n",
    "            else:\n",
    "                size = os.path.getsize(item_path)\n",
    "                print(f\"  [FILE] {item} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(\"  (not created yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e821f8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. Configured streaming sinks with checkpointing\n",
    "2. Wrote transformed data to Parquet with date partitioning\n",
    "3. Wrote aggregated metrics to Parquet\n",
    "4. Monitored streaming query progress\n",
    "5. Verified output data integrity\n",
    "6. Demonstrated graceful query shutdown\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "### Checkpointing\n",
    "- Stores streaming offsets and metadata\n",
    "- Enables exactly-once processing\n",
    "- Required for production streaming\n",
    "- Never delete in production\n",
    "\n",
    "### Output Formats\n",
    "- **Parquet**: Columnar, compressed, efficient for analytics\n",
    "- **Partitioning**: Improves query performance by pruning\n",
    "- **Append Mode**: Best for most ETL scenarios\n",
    "- **Complete Mode**: Required for stateful aggregations without watermarks\n",
    "\n",
    "### Triggers\n",
    "- **Processing Time**: Fixed interval micro-batches\n",
    "- **Once**: Process all available data then stop\n",
    "- **Continuous**: Low-latency processing (experimental)\n",
    "\n",
    "## Production Recommendations\n",
    "\n",
    "1. **Monitoring**: Integrate with monitoring systems (Prometheus, Datadog)\n",
    "2. **Alerting**: Set up alerts for query failures and lag\n",
    "3. **Scaling**: Adjust shuffle partitions based on data volume\n",
    "4. **Watermarking**: Use event-time watermarks for late data handling\n",
    "5. **State Management**: Monitor state store size for memory usage\n",
    "6. **Backpressure**: Configure maxFilesPerTrigger to control ingestion rate\n",
    "7. **Testing**: Test recovery from checkpoint after failures\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Set up orchestration with Airflow or similar\n",
    "- Add data quality checks and validation\n",
    "- Implement alerts and monitoring dashboards\n",
    "- Consider Delta Lake for ACID transactions\n",
    "- Add schema evolution handling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
